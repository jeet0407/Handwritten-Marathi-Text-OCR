{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccf6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ce2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory containing train, val, test folders\n",
    "base_dir = 'pls' \n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "train_labels_csv = os.path.join(train_dir, 'labels.csv')\n",
    "val_labels_csv = os.path.join(val_dir, 'labels.csv')\n",
    "test_labels_csv = os.path.join(test_dir, 'labels.csv')\n",
    "\n",
    "# --- Model & Training Parameters ---\n",
    "IMG_WIDTH = 32\n",
    "IMG_HEIGHT = 32\n",
    "IMG_CHANNELS = 1 \n",
    "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "BATCH_SIZE = 64 \n",
    "EPOCHS = 20     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34860d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_df = pd.read_csv(train_labels_csv)\n",
    "    val_df = pd.read_csv(val_labels_csv)\n",
    "    test_df = pd.read_csv(test_labels_csv)\n",
    "\n",
    "    print(\"Train DataFrame Head:\")\n",
    "    print(train_df.head())\n",
    "    print(f\"\\nTotal Train Samples: {len(train_df)}\")\n",
    "    print(f\"Total Validation Samples: {len(val_df)}\")\n",
    "    print(f\"Total Test Samples: {len(test_df)}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    print(\"Please ensure the root directory and the 'labels.csv' files exist in the correct locations.\")\n",
    "    # Stop execution if files aren't found\n",
    "    raise SystemExit(\"CSV file not found.\")\n",
    "\n",
    "print(\"\\nMissing values in train_df:\", train_df.isnull().sum().sum())\n",
    "print(\"Missing values in val_df:\", val_df.isnull().sum().sum())\n",
    "print(\"Missing values in test_df:\", test_df.isnull().sum().sum())\n",
    "\n",
    "# Ensure column names are correct (adjust if needed)\n",
    "required_columns = ['filename', 'words']\n",
    "if not all(col in train_df.columns for col in required_columns):\n",
    "     raise ValueError(f\"Train CSV must contain columns: {required_columns}\")\n",
    "if not all(col in val_df.columns for col in required_columns):\n",
    "     raise ValueError(f\"Validation CSV must contain columns: {required_columns}\")\n",
    "if not all(col in test_df.columns for col in required_columns):\n",
    "     raise ValueError(f\"Test CSV must contain columns: {required_columns}\")\n",
    "\n",
    "label_column_name = 'words'\n",
    "\n",
    "print(f\"\\nUsing label column: '{label_column_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca40fa2",
   "metadata": {},
   "source": [
    "Train DataFrame Head:\n",
    "                     filename words\n",
    "0  character_10_yna/13839.png     ञ\n",
    "1  character_10_yna/79348.png     ञ\n",
    "2  character_10_yna/23260.png     ञ\n",
    "3  character_10_yna/57417.png     ञ\n",
    "4  character_10_yna/19463.png     ञ\n",
    "\n",
    "Total Train Samples: 73600\n",
    "Total Validation Samples: 9200\n",
    "Total Test Samples: 9200\n",
    "\n",
    "Missing values in train_df: 0\n",
    "Missing values in val_df: 0\n",
    "Missing values in test_df: 0\n",
    "\n",
    "Using label column: 'words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the unique characters across train, val, test sets\n",
    "all_labels = pd.concat([train_df[label_column_name], \n",
    "                        val_df[label_column_name], \n",
    "                        test_df[label_column_name]], ignore_index=True)\n",
    "unique_labels = sorted(all_labels.unique())\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"\\nTotal unique classes (Devanagari characters): {num_classes}\")\n",
    "\n",
    "label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
    "int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "print(int_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068290e",
   "metadata": {},
   "source": [
    "Total unique classes (Devanagari characters): 46\n",
    "{0: 'क', 1: 'क्ष', 2: 'ख', 3: 'ग', 4: 'घ', 5: 'ङ', 6: 'च', 7: 'छ', 8: 'ज', 9: 'ज्ञ', 10: 'झ', 11: 'ञ', 12: 'ट', 13: 'ठ', 14: 'ड', 15: 'ढ', 16: 'ण', 17: 'त', 18: 'त्र', 19: 'थ', 20: 'द', 21: 'ध', 22: 'न', 23: 'प', 24: 'फ', 25: 'ब', 26: 'भ', 27: 'म', 28: 'य', 29: 'र', 30: 'ल', 31: 'व', 32: 'श', 33: 'ष', 34: 'स', 35: 'ह', 36: '०', 37: '१', 38: '२', 39: '३', 40: '४', 41: '५', 42: '६', 43: '७', 44: '८', 45: '९'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca021e6",
   "metadata": {},
   "source": [
    "['क', 'क्ष', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'ज्ञ', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'त्र', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=INPUT_SHAPE),\n",
    "\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),          \n",
    "\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),                                    # Dropout for regularization\n",
    "        layers.Dense(256, activation=\"relu\"),                   # Add a dense layer\n",
    "        layers.Dropout(0.3),                                    # Optional dropout\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),        # Output layer\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aryan idhar summary generate karva k push kardena "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,      # degrees\n",
    "    width_shift_range=0.1,  # fraction of total width\n",
    "    height_shift_range=0.1, # fraction of total height\n",
    "    shear_range=0.1,        # shear intensity (angle in counter-clockwise direction)\n",
    "    zoom_range=0.1,         # random zoom range [1-zoom_range, 1+zoom_range]\n",
    "    fill_mode='nearest'     # strategy used for filling points outside the input boundaries\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# --- Create Generators ---\n",
    "print(\"\\nCreating Training Generator...\")\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=train_dir,  # Directory where images referenced in df are located\n",
    "    x_col='filename',     # Column in df that contains the filenames\n",
    "    y_col=label_column_name,       # Column in df that has the target labels (characters)\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    color_mode='grayscale' if IMG_CHANNELS == 1 else 'rgb', # 'grayscale' or 'rgb'\n",
    "    classes=unique_labels, # Explicitly provide the class labels in sorted order\n",
    "    class_mode='categorical', # Returns 2D one-hot encoded labels\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,       \n",
    "    seed=42               # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\nCreating Validation Generator...\")\n",
    "validation_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=val_dir,\n",
    "    x_col='filename',\n",
    "    y_col=label_column_name,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    color_mode='grayscale' if IMG_CHANNELS == 1 else 'rgb',\n",
    "    classes=unique_labels,\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nCreating Test Generator...\")\n",
    "test_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=test_dir,\n",
    "    x_col='filename',\n",
    "    y_col=label_column_name,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    color_mode='grayscale' if IMG_CHANNELS == 1 else 'rgb',\n",
    "    classes=unique_labels,\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "try:\n",
    "    x_batch, y_batch = next(train_generator)\n",
    "    print(f\"\\nSample batch shape: images={x_batch.shape}, labels={y_batch.shape}\")\n",
    "    # Check image range\n",
    "    print(f\"Image data range: min={np.min(x_batch)}, max={np.max(x_batch)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError fetching batch from train_generator: {e}\")\n",
    "    print(\"Please check image paths in 'pls/train/labels.csv' and ensure images exist and are readable.\")\n",
    "    print(\"Example path expected by generator:\", os.path.join(train_dir, train_df['filename'].iloc[0]))\n",
    "    raise SystemExit(\"Error creating data generator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944e231",
   "metadata": {},
   "source": [
    "\n",
    "Creating Training Generator...\n",
    "Found 73600 validated image filenames belonging to 46 classes.\n",
    "\n",
    "Creating Validation Generator...\n",
    "Found 9200 validated image filenames belonging to 46 classes.\n",
    "\n",
    "Creating Test Generator...\n",
    "Found 9200 validated image filenames belonging to 46 classes.\n",
    "\n",
    "Sample batch shape: images=(64, 32, 32, 1), labels=(64, 46)\n",
    "Image data range: min=0.0, max=1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726b1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_function = \"categorical_crossentropy\"\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',   # Monitor validation accuracy\n",
    "    patience=5,               # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity.\n",
    ")\n",
    "\n",
    "# Reduce learning rate if validation loss plateaus\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2, # Factor by which the learning rate will be reduced. new_lr = lr * factor\n",
    "    patience=3,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_generator) \n",
    "validation_steps = len(validation_generator)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30241e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating on Test Set...\")\n",
    "\n",
    "test_steps = len(test_generator)\n",
    "\n",
    "loss, accuracy = model.evaluate(\n",
    "    test_generator,\n",
    "    steps=test_steps, # Ensure all test samples are evaluated\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aryan idhar bhi test accuracy run karva k dikha dena "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223f7c4",
   "metadata": {},
   "source": [
    "\n",
    "Evaluating on Test Set...\n",
    "144/144 ━━━━━━━━━━━━━━━━━━━━ 7s 51ms/step - accuracy: 0.9857 - loss: 0.0490\n",
    "\n",
    "Test Loss: 0.0420\n",
    "Test Accuracy: 0.9879 (98.79%)\n",
    "\n",
    "yeh wali hata dena nakli lag rhi hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the model later\n",
    "model = tf.keras.models.load_model('first.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
